environment:
  name: GPUQuadcopterTTDecomposition
  eval_envname: QuadcopterTTDecomposition-v0
  num_envs: 646
  normalized_rewards: false
  environment_kwargs:
    trajectory_file: trajectory_sine.mat
    param:
      T: 3.
      dt: 0.004
      max_thrust_factor: 2
    reference_trajectory_horizon: 0.4
    normalized_actions: true
    normalized_observations: true
    intermittent_starts: true
    alpha_cost: 5.693
    alpha_action_cost: 5.693
    alpha_terminal_cost: 0.3166

algorithm:
  name: PPO
  total_timesteps: 30000000
  save_every_timestep: 500000
  max_no_improvement_evals: 60
  min_evals: 4
  improve_tol: -0.1
  eval_offline: true
  n_eval_episodes: 100
  algorithm_kwargs:
    gamma: 0.9984539792726858
    gae_lambda: 0.995
    ent_coef: 0.
    vf_coef: 0.5
    normalize_advantage: false
    max_grad_norm: 0.09753530446987978
    learning_rate: 2.417399226931629e-05
    learning_rate_schedule:
      type: kla
      target_kl: null
    n_steps: 174
    n_epochs: 5
    batch_size: 28101
    clip_range: 0.1

decomposition:
  independent_states: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  in_tree:
    - [-1, 1]
    - [-1, 1]
    - [-1, 1]
    - [-1, 1]
  in_st_map:
    - [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    - [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    - [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    - [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

policy:
  save_prefix: model
  policy_kwargs:
    net_arch:
      pi:
        - - [0,1,2,3]
          - [1024,1024,1024]
    activation_fn: relu
    optimizer_class: rmsprop
    log_std_init: -0.01946188846120455
    ortho_init: true

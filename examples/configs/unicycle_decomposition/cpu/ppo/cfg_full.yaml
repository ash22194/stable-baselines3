environment:
  name: 'UnicycleDecomposition-v0'
  num_envs: 7
  normalized_rewards: False
  environment_kwargs:
    nonlinear_cost: False
    normalized_actions: True
    normalized_observations: True
    intermittent_starts: False
    param:
      dt: 0.005
      T: 4.
    alpha_cost: 31.351
    alpha_action_cost: 31.351
    alpha_terminal_cost: 0.08501

algorithm:
  name: 'PPO'
  total_timesteps: 20000000
  save_every_timestep: 100000
  min_evals: 5
  max_no_improvement_evals: 5
  n_eval_episodes: 30
  algorithm_kwargs:
    gamma: 0.9999
    gae_lambda: 0.99
    ent_coef: 0.
    vf_coef: 0.5
    normalize_advantage: False
    max_grad_norm: 0.9216909281395821
    learning_rate: 0.0009034
    learning_rate_schedule: 
      type: 'lin'
    n_steps: 1036
    n_epochs: 7
    batch_size: null
    clip_range: 0.2

decomposition:
  independent_states: [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]
  in_tree:
    - [-1, 1]
    - [-1, 1]
  in_st_map:
    - [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]
    - [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]

policy:
  save_prefix: 'model'
  type: 'DecompositionMlpPolicy'
  policy_kwargs:
    net_arch:
      pi:
        - - [0, 1]
          - [256,256,256]
      vf: [256, 256, 256]
    activation_fn: 'relu'
    optimizer_class: 'rmsprop'
    log_std_init: -2.067521056483639
    ortho_init: True